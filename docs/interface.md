# Atman Chatbot Interface

Streamlit-based interface for a RAG chatbot with interactive explanations through Atman.

## Core Features

- RAG-powered chatbot with web search integration
- Interactive explanations for model responses
- Model switching and configuration
- Source attribution and verification
- Example prompts for demonstration

## Main Components

### Session State Management

```python
DEFAULT_SESSION_STATE = {
"messages": [], # Chat history
"current_model": "TinyLlama-1.1B", # Active model
"model_handler": None, # LLM instance
"rag": None, # RAG system instance
"selected_sources": [], # Selected context sources
"show_chat": True, # Chat visibility toggle
"pending_example": None # Pending example prompt
}
```

## User Interface Flow

1. **Initialization**
   - Sets up session state
   - Configures model and RAG system
   - Initializes sidebar controls

2. **Input Handling**
   - Accepts user input via chat interface
   - Provides example prompts for demonstration

3. **Response Generation**
   - Retrieves relevant context from web sources
   - Generates model response
   - Displays response with interactive explanations

4. **Context Visualization**
   - Shows source attribution
   - Enables interactive exploration of context
   - Highlights relevant context chunks when text is clicked

## Configuration Options

#### RAG Configuration
- Web search toggle
- Number of web results to retrieve

#### Model Configuration
- Model selection from available models
- Temperature control
- Max token limit
- Random seed setting

#### Explanation Configuration
- Toggle for explanations
- Input chunk mode (word/sentence/paragraph)
- Output chunk mode (word/sentence/paragraph)
- Number of explanation chunks

## Key Functions

### `create_annotated_text(text: str, highlight_chunks: List[str]) -> List[tuple]`
Creates highlighted text segments for visualization of relevant context chunks.

Example:
- **Input:** `text = "This is a sample text."`, `highlight_chunks = ["sample"]`
- **Output:** `[("This is a ", ""), ("sample", "highlight"), (" text.", "")]`

### `create_clickable_text(text: str, chunk_mode: str) -> str`
Generates HTML-formatted clickable text segments based on the specified chunking mode.

Example:
- **Input:** `text = "This is a sample text."`, `chunk_mode = "word"`
- **Output:** `'<span class="clickable">This</span> <span class="clickable">is</span> <span class="clickable">a</span> <span class="clickable">sample</span> <span class="clickable">text.</span>'`

### `handle_click_and_explanations(clicked_text: str, context: str, model_handler)`
Processes click events and generates explanations for selected text using the DocumentQAExplainer.

Example:
- **Input:** `clicked_text = "sample"`, `context = "This is a sample text."`
- **Output:** Explanation generated by DocumentQAExplainer

### `perform_rag_operation(query: str, top_k: int, web_only: bool, seed: int, temperature: float, max_tokens: int) -> tuple`
Executes the RAG pipeline:
1. Retrieves relevant context from web sources
2. Generates a response using the language model
3. Returns response, context, and sources

Example:
- **Input:** `query = "What is AI?"`, `top_k = 5`, `web_only = False`, `seed = 42`, `temperature = 0.7`, `max_tokens = 150`
- **Output:** `(response, context, sources)`

### `clean_text(text: str) -> str`
Sanitizes text by removing HTML tags and normalizing spacing.

Example:
- **Input:** `text = "<p>This is a <b>sample</b> text.</p>"`
- **Output:** `"This is a sample text."`

### `get_delimiters_for_mode(mode: str) -> List[str]`
Returns appropriate text delimiters based on the selected chunking mode.

Example:
- **Input:** `mode = "sentence"`
- **Output:** `[".", "!", "?"]`


## Dependencies

- `streamlit`: Web interface
- `annotated_text`: Streamlit component for text highlighting
- `document_qa_explainer`: Main Interface for Explanation generation using Atman
- `st_click_detector`: Streamlit component for click event handling
- `rag`: Queries either a local vector database or uses web search; saves to local vector database in a defined format
- `torch`: ML framework
- `llama`: Wrapper class for custom llama model

## Usage Notes

1. Requires a Bing API key for web search functionality
2. Supports multiple LLaMA model variants
3. Provides real-time model switching
4. Includes automatic model downloading when needed
5. Supports both CPU and CUDA devices
